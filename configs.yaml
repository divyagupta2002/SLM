model:
  vocab_size: 4096
  embed_dim: 64
  num_heads: 4
  num_layers: 4
  ff_hidden_dim: 128  # Optional, defaults to 2 * embed_dim if not provided
  dropout: 0.1  # Dropout probability for regularization

data:
  tokenizer_path: "tokenizer/bpe_4096.json"
  data_path: "data/100-0.txt"
  context_size: 1024
  batch_size: 256
  val_split: 0.1
  num_workers: 2
  pin_memory: true
  # prefetch_factor: 2

training:
  epochs: 15
  warmup_steps: 100
  learning_rate: 0.01
  grad_clip: 1.0
  grad_accum_steps: 2
  logdir: "runs/experiment_1.4"
  eval_every: 1
  save_every: 5
  checkpoint_path: "checkpoints/model_latest.pth"
  snapshot_path: "checkpoints/model_snapshot.pth"
